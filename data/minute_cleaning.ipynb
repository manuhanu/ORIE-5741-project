{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data and set up original data\n",
    "years = [2017, 2018, 2019, 2020, 2021]\n",
    "df = pd.DataFrame()\n",
    "for y in years:\n",
    "    # might need to adjust file paths\n",
    "    df_y =  pd.read_csv('Bitstamp_BTCUSD_{0}_minute.csv'.format(str(y)),)\n",
    "    df_y = df_y.sort_values('date')\n",
    "    df = df.append(df_y)\n",
    "\n",
    "df = df.drop('symbol',axis = 1)\n",
    "df['dates'] = pd.to_datetime(df['date']).dt.date\n",
    "df['time'] = pd.to_datetime(df['date']).dt.time\n",
    "\n",
    "#find missing dates\n",
    "df_miss = pd.read_csv('BTC-USD.csv')\n",
    "df_miss['Date'] = pd.to_datetime(df_miss['Date'])\n",
    "df_miss.columns = ['date', 'open', 'high', 'low', 'close', 'adj close', 'Volume USD']\n",
    "df_miss = df_miss.set_index('date')\n",
    "df_miss['Volume BTC'] = df_miss['Volume USD'] / df_miss['close']\n",
    "df_miss = df_miss[['open', 'close', 'high', 'low', 'Volume BTC', 'Volume USD']]\n",
    "\n",
    "missing_dates = df_miss.index.tolist()\n",
    "\n",
    "#get bigger dataset\n",
    "bigdata = pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')\n",
    "bigdata=bigdata.drop('Weighted_Price',axis=1)\n",
    "bigdata['datetime'] = pd.to_datetime(bigdata['Timestamp'],unit='s')\n",
    "bigdata['dates'] = pd.to_datetime(bigdata['datetime']).dt.date\n",
    "bigdata['time'] = pd.to_datetime(bigdata['datetime']).dt.time\n",
    "\n",
    "cols = ['Timestamp', 'datetime', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)',\n",
    "        'Volume_(Currency)', 'dates', 'time']\n",
    "\n",
    "bigdata_rearranged = bigdata[cols]\n",
    "\n",
    "missing_dates_df = pd.DataFrame(missing_dates)\n",
    "missing_dates_df['dates'] = pd.to_datetime(missing_dates_df[0]).dt.date\n",
    "\n",
    "missingvals = bigdata_rearranged.loc[bigdata_rearranged['dates'].isin(missing_dates_df['dates'])]\n",
    "\n",
    "missingvals = missingvals.rename(columns=dict(zip(missingvals.columns,bigdata_rearranged.columns)))\n",
    "\n",
    "missingvals = missingvals.rename(columns=dict(zip(missingvals.columns,df.columns)))\n",
    "\n",
    "df = df.append(missingvals)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df = df.sort_values(by=['date'])\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#now lets get the actual technical features we want to use\n",
    "\n",
    "df['SMA200'] = ta.trend.sma_indicator(close=df['close'],window=288000) #200 day simple moving average\n",
    "df['SMA20'] = ta.trend.sma_indicator(close=df['close'],window=28800) #20 day simple moving average\n",
    "\n",
    "df['next_min_change'] = ta.momentum.roc(df['close'],window=1).shift(-1) #minute over minute return\n",
    "\n",
    "df['next_min_upordown'] = np.where(df['next_min_change']>0,1,0) #minute over minute up or down\n",
    "\n",
    "#features related to volume\n",
    "df['change_in_volume'] = df['Volume BTC']- df['Volume BTC'].shift(1) #simple absolute change\n",
    "df['PVO'] = ta.momentum.PercentageVolumeOscillator(df['Volume BTC']).pvo_signal() #percentage volume oscillator, see ta docs\n",
    "#df['PVO_positive'] = np.where(df['PVO']>0,1,0) # variable toget whether pvo was positive or negative. \n",
    "df['ADI'] = ta.volume.AccDistIndexIndicator(df['high'], df['low'], df['close'], df['Volume BTC']).acc_dist_index() #see ta docs\n",
    "df['MFI'] = ta.volume.MFIIndicator(df['high'], df['low'], df['close'], df['Volume BTC'], 30).money_flow_index() #see ta docs\n",
    "df['OBV'] = ta.volume.OnBalanceVolumeIndicator(df['close'], df['Volume BTC']).on_balance_volume() #see ta docs\n",
    "\n",
    "#momentum indicators\n",
    "df['RSI'] = ta.momentum.RSIIndicator(df['close'], 30).rsi() #using last 30 minutes\n",
    "df['TSI'] = ta.momentum.TSIIndicator(df['close'], 30, 15).tsi()\n",
    "\n",
    "#crossover features\n",
    "\n",
    "df['crossed_SMA200_breakdown']= np.where((df['open'] > df['SMA200']) & (df['close'] < df['SMA200']),1,0)#if price goes from above to below\n",
    "df['crossed_SMA200_breakout'] = np.where((df['open'] < df['SMA200']) & (df['close'] > df['SMA200']),1,0)#if price goes from below to above\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.set_index(df['date'])\n",
    "dataset = dataset.dropna()\n",
    "X = dataset.drop(['date','unix','open','high','low','close','Volume BTC','Volume USD','dates','time','next_min_change','next_min_upordown','SMA200', 'SMA20'],axis = 1)\n",
    "\n",
    "y = dataset['next_min_upordown']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71871, 9) (71871,)\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.read_csv('btc_test.csv',header=1)\n",
    "testdata['dates'] = pd.to_datetime(testdata['date']).dt.date\n",
    "testdata['time'] = pd.to_datetime(testdata['date']).dt.time\n",
    "\n",
    "testdata = testdata.reindex(index=testdata.index[::-1])\n",
    "\n",
    "#now lets get the actual technical features we want to use\n",
    "\n",
    "testdata['SMA200'] = ta.trend.sma_indicator(close=testdata['close'],window=288000) #200 day simple moving average\n",
    "testdata['SMA20'] = ta.trend.sma_indicator(close=testdata['close'],window=28800) #20 day simple moving average\n",
    "\n",
    "testdata['next_min_change'] = ta.momentum.roc(testdata['close'],window=1).shift(-1) #minute over minute return\n",
    "\n",
    "testdata['next_min_upordown'] = np.where(testdata['next_min_change']>0,1,0) #minute over minute up or down\n",
    "\n",
    "#features related to volume\n",
    "testdata['change_in_volume'] = testdata['Volume BTC']- testdata['Volume BTC'].shift(1) #simple absolute change\n",
    "testdata['PVO'] = ta.momentum.PercentageVolumeOscillator(testdata['Volume BTC']).pvo_signal() #percentage volume oscillator, see ta docs\n",
    "#testdata['PVO_positive'] = np.where(testdata['PVO']>0,1,0) # variable toget whether pvo was positive or negative. \n",
    "testdata['ADI'] = ta.volume.AccDistIndexIndicator(testdata['high'], testdata['low'], testdata['close'], testdata['Volume BTC']).acc_dist_index() #see ta docs\n",
    "testdata['MFI'] = ta.volume.MFIIndicator(testdata['high'], testdata['low'], testdata['close'], testdata['Volume BTC'], 30).money_flow_index() #see ta docs\n",
    "testdata['OBV'] = ta.volume.OnBalanceVolumeIndicator(testdata['close'], testdata['Volume BTC']).on_balance_volume() #see ta docs\n",
    "\n",
    "#momentum indicators\n",
    "testdata['RSI'] = ta.momentum.RSIIndicator(testdata['close'], 30).rsi() #using last 30 minutes\n",
    "testdata['TSI'] = ta.momentum.TSIIndicator(testdata['close'], 30, 15).tsi()\n",
    "\n",
    "#crossover features\n",
    "\n",
    "testdata['crossed_SMA200_breakdown']= np.where((testdata['open'] > testdata['SMA200']) & (testdata['close'] < testdata['SMA200']),1,0)#if price goes from above to below\n",
    "testdata['crossed_SMA200_breakout'] = np.where((testdata['open'] < testdata['SMA200']) & (testdata['close'] > testdata['SMA200']),1,0)#if price goes from below to above\n",
    "\n",
    "dataset1 = testdata.set_index(testdata['date'])\n",
    "dataset1 = dataset1.dropna()\n",
    "\n",
    "\n",
    "\n",
    "dataset1 = dataset1.loc['2021-10-13 08:19:00':,:]\n",
    "\n",
    "xTe= dataset1.drop(['date','unix','open','high','low','close','Volume BTC','Volume USD','dates','time','next_min_change','next_min_upordown','SMA200', 'SMA20','symbol'],axis = 1)\n",
    "\n",
    "yTe = dataset1['next_min_upordown']\n",
    "\n",
    "print(xTe.shape,yTe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into train and validation using sklearn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "ts_cv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:     0.544 +/- 0.016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets try gradient boosting since its an ensemble method that can handle categorical and numerical values\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "gbclassifier = HistGradientBoostingClassifier()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "pipeline = make_pipeline(scaler, gbclassifier)\n",
    "\n",
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring= [\"accuracy\"]\n",
    "    )\n",
    "    acc = cv_results[\"test_accuracy\"]\n",
    "    print(f\"mean:     {acc.mean():.3f} +/- {acc.std():.3f}\\n\")\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "defaulthgb = evaluate(pipeline, X, y, cv=ts_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69160581 -0.69058307 -0.68970181 -0.68897041 -0.68830588 -0.68774386\n",
      " -0.68725144 -0.68682237 -0.6864369  -0.68608035 -0.68575792 -0.68547524\n",
      " -0.68522779 -0.6850043  -0.6848073  -0.68461835 -0.68445669 -0.68428711\n",
      " -0.68415486 -0.68402416 -0.6839052  -0.68377512 -0.68366037 -0.68356659\n",
      " -0.68348402 -0.68339125 -0.68331892 -0.68324154 -0.68316824 -0.6831057\n",
      " -0.68303831 -0.68297837 -0.68291977 -0.68286465 -0.68281473 -0.68274154\n",
      " -0.68268935 -0.68264835 -0.68260877 -0.6825686  -0.68252159 -0.6824884\n",
      " -0.6824474  -0.68240787 -0.68237499 -0.68233258 -0.68227262 -0.68224038\n",
      " -0.68220739 -0.68217732 -0.68215069 -0.68212399 -0.68209685 -0.68207241\n",
      " -0.68205012 -0.68200317 -0.68197811 -0.68195368 -0.68193451 -0.68191359\n",
      " -0.68189114 -0.68186508 -0.6818451  -0.68182029 -0.6817961  -0.68175628\n",
      " -0.68173423 -0.68172045 -0.68170335 -0.6816819  -0.68165723 -0.68163325\n",
      " -0.68161813 -0.68158867 -0.68157319 -0.68155349 -0.68153469 -0.68151797\n",
      " -0.68150337 -0.68148628 -0.68146854 -0.68145478 -0.68143676 -0.68141954\n",
      " -0.68140959 -0.68138766 -0.68137663 -0.68136239 -0.68134328 -0.68131809\n",
      " -0.68129959 -0.68128875 -0.68127889 -0.68126148 -0.68124177 -0.68122484\n",
      " -0.68121018 -0.68119733 -0.68118886 -0.68116731 -0.68115136]\n"
     ]
    }
   ],
   "source": [
    "gb = gbclassifier.fit(X,y)\n",
    "\n",
    "score = gb.train_score_\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to tune model parameters\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gbclassifier = HistGradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'max_iter': [1200],\n",
    "    'learning_rate': [.05,0.1],\n",
    "    'max_depth' : [25, 50, 75],\n",
    "    'l2_regularization': [.5,1.5,2],\n",
    "    'scoring': ['accuracy']\n",
    "}\n",
    "#start with gridsearch\n",
    "hgb_grid = GridSearchCV(gbclassifier, parameters, n_jobs=5, cv=ts_cv, scoring='accuracy',refit=True)\n",
    "\n",
    "hgb_grid.fit(X, y)\n",
    "\n",
    "print(hgb_grid.best_params_)\n",
    "# Print the best scores found\n",
    "print()\n",
    "print(hgb_grid.best_score_)\n",
    "\n",
    "\n",
    "#THIS TAKES APROX 20-25 MINS TO RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scoring': 'accuracy', 'max_iter': 1200, 'max_depth': 75, 'learning_rate': 0.0775, 'l2_regularization': 1.5}\n",
      "\n",
      "0.5448555510625871\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "gbclassifier = HistGradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'max_iter': [1200],\n",
    "    'learning_rate': np.linspace(0.01,.1,5),\n",
    "    'max_depth' : [25, 50, 75],\n",
    "    'l2_regularization': [.5,1.5,2],\n",
    "    'scoring': ['accuracy'],\n",
    "    'warm_start': True\n",
    "    \n",
    "}\n",
    "#start with gridsearch\n",
    "hgb_rand = RandomizedSearchCV(gbclassifier, parameters, n_jobs=5, cv=ts_cv, scoring='accuracy',refit=True)\n",
    "\n",
    "hgb_rand.fit(X, y)\n",
    "\n",
    "print(hgb_rand.best_params_)\n",
    "# Print the best scores found\n",
    "print()\n",
    "print(hgb_rand.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.556616794716269\n",
      "test acc:  0.5133503081910645\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ', hgb_rand.score(X,y))\n",
    "print('test acc: ', hgb_rand.score(xTe,yTe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': np.linspace(200,1000,5,dtype=int),\n",
    "    'max_depth' : [3,10,20],\n",
    "    #'scoring': ['accuracy'],\n",
    "    'warm_start': [True]\n",
    "}\n",
    "\n",
    "rfc_rand = RandomizedSearchCV(rfc, parameters, n_jobs=5, cv=ts_cv, scoring='accuracy',refit=True)\n",
    "rfc_rand.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.5392568630093904\n",
      "test acc:  0.516146985571371\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ', rfc_rand.score(X,y))\n",
    "print('test acc: ', rfc_rand.score(xTe,yTe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c5ad049284fdd8572b5535696a8e5071906ef1ed2d33a795816e1fff3da542c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
